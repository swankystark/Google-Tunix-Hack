{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.12.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"tpuV5e8","dataSources":[{"sourceId":119261,"databundleVersionId":14363498,"sourceType":"competition"}],"dockerImageVersionId":31194,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# Install Tunix\n!pip install -q git+https://github.com/google/tunix\n!pip install -q git+https://github.com/google/qwix\n!pip uninstall -q flax -y && pip install -q git+https://github.com/google/flax\n\n# Imports\nimport jax\nimport jax.numpy as jnp\nfrom flax import nnx\nimport grain\nimport optax\nfrom tunix.models.gemma3 import model as gemma_lib\nfrom tunix.models.gemma3 import params_safetensors as params_safetensors_lib\nfrom tunix.rl import rl_cluster as rl_cluster_lib\nfrom tunix.rl.grpo.grpo_learner import GRPOConfig, GRPOLearner\nfrom tunix.rl.rollout import base_rollout\nfrom tunix.generate import tokenizer_adapter as tokenizer_lib\nimport qwix\nfrom huggingface_hub import snapshot_download\nimport kagglehub\nimport csv\nimport re\n\n# V1: TPU Check\nprint(f\"JAX devices: {jax.devices()}\")\nassert jax.device_count() > 0, \"No TPU found!\"\n\n# Config (V4/V5 safe values)\nTRAIN_MICRO_BATCH_SIZE = 2  # V4: Safe batch size\nNUM_ITERATIONS = 1  # V5: Safe iteration count\nNUM_BATCHES = 5000\nRANK = 64\nREASONING_WEIGHT = 0.6  # COMPETITIVE ADVANTAGE!\nANSWER_WEIGHT = 0.4\n\n# Dataset\nkaggle_path = kagglehub.competition_download(\"google-tunix-hack\")\ndata = []\nwith open(f\"{kaggle_path}/main_train.csv\") as f:\n    for row in csv.DictReader(f):\n        data.append({\"question\": row[\"question\"], \"answer\": row[\"answer\"].split(\"####\")[-1].strip()})\n\ndataset = grain.MapDataset.source(data[:NUM_BATCHES]).batch(TRAIN_MICRO_BATCH_SIZE)\n\n# Model\nMODEL_ID = \"google/gemma-2-2b-it\"\nlocal_model_path = snapshot_download(repo_id=MODEL_ID, ignore_patterns=[\"*.pth\"])\nmodel_config = gemma_lib.ModelConfig.gemma2_2b()\nmesh = jax.make_mesh((1, 4), (\"fsdp\", \"tp\"))\n\nwith mesh:\n    gemma2 = params_safetensors_lib.create_model_from_safe_tensors(local_model_path, model_config, mesh)\n\n# LoRA\nlora_provider = qwix.LoraProvider(module_path=\".*q_einsum|.*kv_einsum|.*gate_proj|.*down_proj|.*up_proj\", rank=RANK, alpha=64.0)\nlora_policy = qwix.apply_lora_to_model(gemma2, lora_provider, **gemma2.get_model_input())\n\n# COMPETITIVE ADVANTAGE: Trajectory Reward\ndef trajectory_reward(prompts, completions, answer, **kwargs):\n    rewards = []\n    for comp, ref in zip(completions, answer):\n        # Reasoning quality\n        reasoning = re.search(r'<reasoning>(.*?)</reasoning>', comp, re.DOTALL)\n        r_score = 0.5 if reasoning and len(reasoning.group(1).split()) > 20 else 0.0\n        \n        # Answer correctness\n        ans = re.search(r'<answer>(.*?)</answer>', comp, re.DOTALL)\n        a_score = 1.0 if ans and ans.group(1).strip() == ref else 0.0\n        \n        # 60/40 split!\n        reward = (REASONING_WEIGHT * r_score + ANSWER_WEIGHT * a_score) * 3.0\n        rewards.append(reward)\n    return rewards\n\n# GRPO\noptimizer = optax.adamw(learning_rate=5e-5)\ncluster_config = rl_cluster_lib.ClusterConfig(\n    role_to_mesh={rl_cluster_lib.Role.ACTOR: mesh, rl_cluster_lib.Role.REFERENCE: mesh, rl_cluster_lib.Role.ROLLOUT: mesh},\n    rollout_engine='vanilla',\n    training_config=rl_cluster_lib.RLTrainingConfig(\n        actor_optimizer=optimizer,\n        max_steps=NUM_BATCHES,\n        mini_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        train_micro_batch_size=TRAIN_MICRO_BATCH_SIZE,\n        checkpoint_root_directory=\"/kaggle/working/checkpoints\"\n    ),\n    rollout_config=base_rollout.RolloutConfig(max_tokens_to_generate=768, max_prompt_length=256)\n)\n\ngrpo_config = GRPOConfig(num_generations=2, num_iterations=NUM_ITERATIONS, beta=0.08, epsilon=0.2)\n\ntokenizer = tokenizer_lib.Tokenizer(tokenizer_path=\"gs://gemma-data/tokenizers/tokenizer.model\")\nrl_cluster = rl_cluster_lib.RLCluster(actor=lora_policy, reference=gemma2, tokenizer=tokenizer, cluster_config=cluster_config)\ngrpo_trainer = GRPOLearner(rl_cluster=rl_cluster, reward_fns=[trajectory_reward], grpo_config=grpo_config)\n\n# Train!\nprint(\"ðŸš€ Starting training with trajectory reward (60% reasoning + 40% answer)...\")\nwith mesh:\n    grpo_trainer.train(dataset, None)\n\nprint(\"âœ… Training complete!\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null}]}