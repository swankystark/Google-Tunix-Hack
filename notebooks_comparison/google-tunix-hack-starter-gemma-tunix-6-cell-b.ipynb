{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This notebook provides a simple 6‑cell starter baseline for the Google Tunix Hackathon.  \nIt demonstrates how to load the competition data, initialize Gemma models with Tunix, run a basic GRPO training loop, and generate outputs in the required <reasoning>…</reasoning> and <answer>…</answer> format.  \n\nDesigned for educational purposes — not guaranteed to reach medal level.  \nParticipants are encouraged to extend the reward function, adjust hyperparameters, and refine the reasoning trace generation to improve performance.  \n","metadata":{}},{"cell_type":"code","source":"# =========================\n# Cell 1 — Imports & Config\n# =========================\nimport os, random\nimport jax, flax, optax\nimport tunix\nfrom transformers import AutoTokenizer, FlaxAutoModelForCausalLM\n\nSEED = 42\nrandom.seed(SEED)\nMODEL_NAME = \"google/gemma-2b\"   # or \"google/gemma-3b\"\nDEVICE = jax.devices()[0]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 2 — Load Data\n# =========================\nimport pandas as pd\nDATA_DIR = \"/kaggle/input/google-tunix-hackathon\"\ntrain = pd.read_csv(os.path.join(DATA_DIR, \"train.csv\"))\nprint(\"Train shape:\", train.shape)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 3 — Tokenizer & Model\n# =========================\ntokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\nmodel = FlaxAutoModelForCausalLM.from_pretrained(MODEL_NAME)\n\ndef format_prompt(question):\n    return f\"Question: {question}\\nPlease show reasoning and final answer.\"\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 4 — Training Loop (Skeleton)\n# =========================\n# Tunix GRPO training skeleton\nreward_fn = tunix.rewards.basic_trace_reward\ntrainer = tunix.Trainer(\n    model=model,\n    tokenizer=tokenizer,\n    reward_fn=reward_fn,\n    learning_rate=1e-5,\n    num_train_steps=1000\n)\n\n# Example: train on a small batch\nsample_batch = [format_prompt(q) for q in train[\"question\"].head(8)]\ntrainer.train_step(sample_batch)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 5 — Inference\n# =========================\ndef solve(query):\n    inputs = tokenizer(format_prompt(query), return_tensors=\"jax\")\n    outputs = model.generate(**inputs, max_length=256)\n    text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return text\n\nprint(solve(\"What is 2+2?\"))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# =========================\n# Cell 6 — Submission (Reasoning + Answer)\n# =========================\npreds = []\nfor _, row in train.head(5).iterrows():  # demo only\n    out = solve(row[\"question\"])\n    preds.append(out)\n\nsubmission = pd.DataFrame({\n    \"id\": train[\"id\"].head(5),\n    \"output\": preds\n})\nsubmission.to_csv(\"submission.csv\", index=False)\nprint(\"✅ submission.csv saved\")","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}